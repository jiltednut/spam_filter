{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Liam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Liam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import bz2\n",
    "import tarfile\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Language processing\n",
    "import string\n",
    "\n",
    "import enchant\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# URL libs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to specify where to get the email data (lib_url) and where to save it (main_dir, sub_dir)\n",
    "# These will be referenced in the data curation phase of execution.\n",
    "lib_url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "main_dir = \"data\"\n",
    "sub_dir = \"extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure(main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to create the directory structure on disk in the case that it doesn't already exist\n",
    "    The directory will be created in the same directory as the source file and will use the structure main_dir/sub_dir\n",
    "    Input:\n",
    "        main_dir: the top level of the directory structure\n",
    "        sub_dir: sublevel in the directory structure\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(\".\\\\\" + main_dir)\n",
    "    except:\n",
    "        print(\"Directory already exists.\")\n",
    "        try:\n",
    "            os.mkdir(\".\\\\\"+ main_dir +\"\\\\\"+ sub_dir +\"\\\\\")\n",
    "        except:\n",
    "            print(\"Directory already exists.\")\n",
    "    print(\"Directory Structure Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_email_records(url):\n",
    "    \"\"\"\n",
    "    Function to download the page source from online directory and create a list of filenames with .tar.bz2 extensions\n",
    "    Input:\n",
    "        url: the lib_url defined in the source\n",
    "    Returns:\n",
    "        downloadable: a list of urls made of the lib_url web address and the filenames extracted from the hyperlines in the page source.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(lib_url).text)\n",
    "    urls = soup.find_all('a')\n",
    "    filenames = [url['href'] for url in urls if \"bz2\" in str(url)]\n",
    "    downloadable = [lib_url + filename for filename in filenames]\n",
    "    print(\"Email archive urls extracted\")\n",
    "    return downloadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_email_records(file_urls):\n",
    "    \"\"\"\n",
    "    Function to download the email archives and write them to disk in the directory hierarchy\n",
    "    Input:\n",
    "        file_urls: a list of urls pointing to the email archives\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    for i, url in enumerate(file_urls):\n",
    "        dl = requests.get(url, allow_redirects = True)\n",
    "        open(\".//\" + main_dir + \"//\"+file_urls[i].split(\"/\")[-1], 'wb').write(dl.content)\n",
    "    print(\"Email archives downloaded from url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_records(main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to extract the email records from the downloaded email archives.\n",
    "    Loops through all files in each level of the directory hierarchy and extracts the data from all tar.bz2 archives to disk.\n",
    "    Input:\n",
    "        main_dir: the top level of the directory structure\n",
    "        sub_dir: sublevel in the directory structure\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    for filepath in glob.glob(\".\\\\\" + main_dir + \"\\\\*.tar.bz2\"):\n",
    "        #zipfile = bz2.BZ2File(filepath)\n",
    "        #data = zipfile.read()\n",
    "        #newfile = filepath[:-4]\n",
    "        #open(newfile, \"wb\").write(data)\n",
    "        tar = tarfile.open(filepath, \"r:bz2\")\n",
    "        tar.extractall(os.path.join(main_dir+ \"\\\\\"+ sub_dir, filepath[7:-8]))\n",
    "        tar.close()\n",
    "    print(\"Email records extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_and_create_email_records(url, main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Wrapper function to download and create the email records from the archive.\n",
    "    \"\"\"\n",
    "    urls = download_email_records(url)\n",
    "    create_directory_structure(main_dir = main_dir, sub_dir = sub_dir)\n",
    "    save_email_records(file_urls = urls)\n",
    "    extract_email_records(main_dir = main_dir, sub_dir = sub_dir)\n",
    "    print(\"Email records downloaded & extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_directory_details(target_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to traverse a directory and record the target type of each folder by checking if the folder\n",
    "    contains either \"HAM\" or \"SPAM\" in the name.\n",
    "    Input:\n",
    "        target_dir: the target directory\n",
    "        sub_dir: the sub directory\n",
    "    Returns:\n",
    "        email_type_names: a list containing the folder path, in the sub_dir, and the target type based on the folder name.\n",
    "    \"\"\"\n",
    "    sub_directories = glob.glob(target_dir + \"\\\\\"+ sub_dir +\"\\\\*\\\\*\")\n",
    "#     print(target_dir)\n",
    "#     print(os.path.join(target_dir, \"\\\\extracted\\\\*\\\\*\"))\n",
    "#     print(sub_directories)\n",
    "    names = [(x.split(\"\\\\\")[-1], \"HAM\" if x.find(\"ham\") >=0 else \"SPAM\") for x in sub_directories]\n",
    "    email_type_names = list(zip(names, sub_directories))\n",
    "    print(\"Target directories extracted\")\n",
    "    return email_type_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(email, line_names, target):\n",
    "    \"\"\"\n",
    "    Function to take in the filename of an email document.\n",
    "    Extract any information relating to the predefined tags\n",
    "    Store any information after the subject line as body - to be further processed later\n",
    "    Input:\n",
    "        email: the email text file, extracted from the email archive\n",
    "        line_names: a predefined list of line start strings that will correspond to column headers later\n",
    "        target: the target type extracted from the home folder of the email file.\n",
    "    Returns:\n",
    "        value_dict: a dictionary with the extracted body text, target type and key-value pairs for the line_names values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(email) as file:\n",
    "            body_start = False # Changed to True after reading the subject tag.\n",
    "            body = []\n",
    "            value_dict = {}\n",
    "            value_dict['target'] = target\n",
    "\n",
    "            for line in file.readlines():\n",
    "                line_start = line.split(\":\")[0]+\":\"\n",
    "                if body_start:\n",
    "                    body.append(line.strip())\n",
    "                if line_start in line_names:\n",
    "                    line_contents = re.findall(r\":\\s(.*)\", line)[0]\n",
    "                    value_dict[line_start] = line_contents\n",
    "                if line_start == \"Subject:\":\n",
    "                    body_start = True\n",
    "            value_dict['body'] = \"\\n\".join(body)\n",
    "            return value_dict\n",
    "    except Exception as e:\n",
    "        print(f\"{e}: Error: Can't read file {email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_target_mappings(main_dir):\n",
    "    \"\"\"\n",
    "    Function to map the target type to the folder name.\n",
    "    Used to map the target to the individual email text files later.\n",
    "    Input:\n",
    "        main_dir: The directory that needs to be mapped\n",
    "    Returns:\n",
    "        target_mapping: list of tuples containing the folder path & the target type (\"HAM\" or \"SPAM\").    \n",
    "    \"\"\"\n",
    "    email_type_names = get_target_directory_details(\".\\\\\" + main_dir, sub_dir)\n",
    "    directories = [x[1] for x in email_type_names]\n",
    "    targets = [x[0][1] for x in email_type_names]\n",
    "    target_mapping = list(zip(directories, targets))\n",
    "    print(\"Target mappings extracted\")\n",
    "    return target_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_file_listing(dir_path):\n",
    "    \"\"\"\n",
    "    Function to create a list of all the file paths in a directory\n",
    "    Input:\n",
    "        dir_path: the file path of a directory\n",
    "    Returns:\n",
    "        a list of all files in the directory.\n",
    "    \"\"\"\n",
    "    print(dir_path + \"\\\\\")\n",
    "    return glob.glob(dir_path + \"\\\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_data_to_dictionary(main_dir):\n",
    "    \"\"\"\n",
    "    Function to extract the details from the email text files and store in a dictionary\n",
    "    Inputs:\n",
    "        main_dir: the directory containing the email text files\n",
    "    Returns:\n",
    "        email_contents: dictionary containing the extracted dictionaries from the function parse_email().\n",
    "    \"\"\"\n",
    "    line_names = [\"To:\", \"From:\", \"MIME-Version:\", \"Content-Type:\",\n",
    "                 \"Content-Transfer-Encoding:\", \"X-Mailer:\", \"Subject:\",\n",
    "                 \"Precedence:\"]\n",
    "\n",
    "    target_mapping = get_email_target_mappings(main_dir = main_dir)\n",
    "    email_contents = {}\n",
    "    for target in target_mapping:\n",
    "        for file in get_directory_file_listing(target[0]):\n",
    "            email_contents[file.split(\"\\\\\")[-1]] = parse_email(file, line_names, target[1])\n",
    "    print(\"Emails extracted to dictionary\")\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(email_dict):\n",
    "    \"\"\"\n",
    "    Function to convert a dictionary to a dataframe and tranpose the resulting dataframe.\n",
    "    Input:\n",
    "        email_dict: a dictionary containing dictionaries with extracted email information\n",
    "    Returns:\n",
    "        df: dataframe generated from the dictionary, transposed to keep keys as the columns and not as the rows.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(email_dict).transpose().reset_index()\n",
    "    print(\"DataFrame generated\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_email_dataframe():\n",
    "    \"\"\"\n",
    "    Wrapper function to return a dataframe from the extracted email archives\n",
    "    \"\"\"\n",
    "    email_dict = extract_email_data_to_dictionary(main_dir = main_dir)\n",
    "    print(\"Base dataframe ready for cleansing\")\n",
    "    return convert_dict_to_dataframe(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target directories extracted\n",
      "Target mappings extracted\n",
      ".\\data\\extracted\\20021010_easy_ham\\easy_ham\\\n",
      ".\\data\\extracted\\20021010_hard_ham\\hard_ham\\\n",
      ".\\data\\extracted\\20021010_spam\\spam\\\n",
      "'charmap' codec can't decode byte 0x81 in position 3082: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0123.68e87f8b736959b1ab5c4b5f2ce7484a\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0255.42a6feb4435a0a68929075c0926f085d\n",
      "'charmap' codec can't decode byte 0x81 in position 2588: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0273.51c482172b47ce926021aa7cc2552549\n",
      "'charmap' codec can't decode byte 0x81 in position 2503: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0330.a4df526233e524104c3b3554dd8ab5a8\n",
      "'charmap' codec can't decode byte 0x81 in position 2682: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0334.3e4946e69031f3860ac6de3d3f27aadd\n",
      "'charmap' codec can't decode byte 0x81 in position 2728: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0335.9822e1787fca0741a8501bdef7e8bc79\n",
      ".\\data\\extracted\\20030228_easy_ham\\easy_ham\\\n",
      ".\\data\\extracted\\20030228_easy_ham_2\\easy_ham_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_easy_ham_2\\easy_ham_2\\00664.28f4cb9fad800d0c7175d3a67e6c6458\n",
      ".\\data\\extracted\\20030228_hard_ham\\hard_ham\\\n",
      ".\\data\\extracted\\20030228_spam\\spam\\\n",
      "'charmap' codec can't decode byte 0x81 in position 3124: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00116.29e39a0064e2714681726ac28ff3fdef\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00245.f129d5e7df2eebd03948bb4f33fa7107\n",
      "'charmap' codec can't decode byte 0x81 in position 2568: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00263.13fc73e09ae15e0023bdb13d0a010f2d\n",
      "'charmap' codec can't decode byte 0x81 in position 2483: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00320.20dcbb5b047b8e2f212ee78267ee27ad\n",
      "'charmap' codec can't decode byte 0x81 in position 2662: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00323.9e36bf05304c99f2133a4c03c49533a9\n",
      "'charmap' codec can't decode byte 0x81 in position 2708: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00324.6f320a8c6b5f8e4bc47d475b3d4e86ef\n",
      "'charmap' codec can't decode byte 0x8f in position 1674: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00500.85b72f09f6778a085dc8b6821965a76f\n",
      ".\\data\\extracted\\20030228_spam_2\\spam_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\00721.09d243c9c4da88c5f517003d26196aaa\n",
      "'charmap' codec can't decode byte 0x8d in position 3062: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01065.9ecef01b01ca912fa35453196b4dae4c\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01083.a6b3c50be5abf782b585995d2c11176b\n",
      "'charmap' codec can't decode byte 0x90 in position 2832: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01227.04a4f94c7a73b29cb56bf38c7d526116\n",
      "'charmap' codec can't decode byte 0x9d in position 4099: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01376.73e738e4cd8121ce3dfb42d190b193c9\n",
      ".\\data\\extracted\\20050311_spam_2\\spam_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\00721.09d243c9c4da88c5f517003d26196aaa\n",
      "'charmap' codec can't decode byte 0x8d in position 3062: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01065.9ecef01b01ca912fa35453196b4dae4c\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01083.a6b3c50be5abf782b585995d2c11176b\n",
      "'charmap' codec can't decode byte 0x90 in position 2832: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01227.04a4f94c7a73b29cb56bf38c7d526116\n",
      "'charmap' codec can't decode byte 0x9d in position 4099: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01376.73e738e4cd8121ce3dfb42d190b193c9\n",
      "Emails extracted to dictionary\n",
      "Base dataframe ready for cleansing\n",
      "DataFrame generated\n"
     ]
    }
   ],
   "source": [
    "#dl_and_create_email_records(lib_url, main_dir, sub_dir)\n",
    "base_email_df = generate_base_email_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_email_df.to_parquet(\"base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_email_df = pd.read_parquet(\"base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_email_df = deepcopy(base_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9350 entries, 0 to 9349\n",
      "Data columns (total 11 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   index                       9350 non-null   object\n",
      " 1   target                      9331 non-null   object\n",
      " 2   From:                       9329 non-null   object\n",
      " 3   To:                         9006 non-null   object\n",
      " 4   Subject:                    9322 non-null   object\n",
      " 5   MIME-Version:               6208 non-null   object\n",
      " 6   Content-Type:               8052 non-null   object\n",
      " 7   Precedence:                 5304 non-null   object\n",
      " 8   body                        9331 non-null   object\n",
      " 9   X-Mailer:                   3650 non-null   object\n",
      " 10  Content-Transfer-Encoding:  4604 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 803.6+ KB\n"
     ]
    }
   ],
   "source": [
    "cleansed_email_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_remove_colon_from_column_name(df):\n",
    "    \"\"\"\n",
    "    Function to remove the colon from the column headers and force the text to lowercase\n",
    "    Input:\n",
    "        df: the target dataframe\n",
    "    Returns:\n",
    "        df: df with renamed columns\n",
    "    \"\"\"\n",
    "    df.columns = [x.replace(\":\", \"\").lower() for x in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_components_to_features(df, user_types):\n",
    "    \"\"\"\n",
    "    Function to extract components of the to & from columns to new features\n",
    "        fullname: the fullname of the sender that prefixs the email address\n",
    "        email: the full email address contained in '<email_address>'\n",
    "        username: the username from the email address (everything before @)\n",
    "        domain: the domain of the email address (everything after @)\n",
    "    Inputs:\n",
    "        df: the target dataframe\n",
    "        user_types: list of types of user that will be processed i.e. ['to'], ['from'], ['to', 'from']\n",
    "    Returns:\n",
    "        df: df with additional features added.\n",
    "    \"\"\"\n",
    "    for user_type in user_types:\n",
    "        # Split the FROM column into full name, username and domain\n",
    "        # df[str(user_type + '_fullname')] = df[str(user_type)].str.split(\"<\", n = 1).str[0].str.replace('\"', \"\")\n",
    "        df[str(user_type + '_fullname')] = df[str(user_type)].str.extract(r'[$\\s\\\"]?([\\w\\d\\s]*)[\\s\\\"]')[0]\n",
    "\n",
    "        # Extract the from email\n",
    "        # df[str(user_type + '_email')] = df[str(user_type)].str.split(\"<\").str[1].str.replace('>', \"\")\n",
    "        # df[str(user_type + '_email')] = df[str(user_type)].str.extract(r'[\\s<]?([\\w\\d\\+]*@.*\\.[\\w\\d]*)')[0]\n",
    "        df[str(user_type + '_email')] = df[str(user_type)].str.extract(r'([\\w\\d\\+]+@[\\w\\d]+\\.[\\w\\d]+)')[0]\n",
    "        df[str(user_type + '_email_count')] = df[str(user_type)].str.count(r'([\\w\\d\\+]+@[\\w\\d]+\\.[\\w\\d]+)')\n",
    "        \n",
    "        # Extract the from username\n",
    "        df[str(user_type + '_username')] = df[str(user_type + '_email')].str.extract(r'(.*)[@]')\n",
    "\n",
    "        # Extract the from domain\n",
    "        df[str(user_type + '_domain')] = df[str(user_type + '_email')].str.extract(r'[@](.*)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_invalid_to_from_subject_target_records(df):\n",
    "    \"\"\"\n",
    "    Function to exclude records with invalid target, to, from & subject.\n",
    "    Input:\n",
    "        df: email contents dataframe\n",
    "    Returns:\n",
    "        df: email contents dataframe without invaid target, to, from & subject rows.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df[df['target'].notna()]\n",
    "    df = df[df['to'].notna()]\n",
    "    df = df[df['from'].notna()]\n",
    "    df = df[df['subject'].notna()]\n",
    "    df = df[df['to_email'].notna()]\n",
    "    df = df[df['from_email'].notna()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_type_info_from_content_type_records(df):\n",
    "    \"\"\"\n",
    "    Function to extract format, type, encoding & character set information from the content-type string\n",
    "    Input:\n",
    "        df: email contents dataframe\n",
    "    Returns:\n",
    "        df: email contents dataframe with additional columns for content-type data\n",
    "    \n",
    "    \"\"\"\n",
    "    df['content-type-format'] = df['content-type'].str.lower().str.extract(r'^(\\w+)/')\n",
    "    df['content-type-type'] = df['content-type'].str.lower().str.extract(r'^\\w+/(\\w+)[;\\s]?')\n",
    "    df['content-type-charset'] = df['content-type'].str.lower().str.extract(r'charset[\\s]?=[\\\"]?([\\w\\d-]+)[\\\"\\s]?')\n",
    "    df['content-type-encoding'] = df['content-type'].str.lower().str.extract(r'encoding[\\s]?=[\\\"]?([\\w\\d-]+)[\\\"\\s]?')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try functions\n",
    "cleansed_email_df = deepcopy(base_email_df)\n",
    "cleansed_email_df = rename_columns_remove_colon_from_column_name(cleansed_email_df)\n",
    "cleansed_email_df = extract_email_components_to_features(cleansed_email_df, ['to', 'from'])\n",
    "cleansed_email_df = exclude_invalid_to_from_subject_target_records(cleansed_email_df)\n",
    "cleansed_email_df = extract_content_type_info_from_content_type_records(cleansed_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop after review\n",
    "# mime-version: no appreciable relevance - all values are 1 with an insignificant qty including additional info (approx 2%)\n",
    "# content-type: feature extraction is complete\n",
    "# Precedence: no appreciable relevance - no alignment between bulk and to_email_count and no obvious way to infer type. Possibly revisit or attempt to create a feature independently\n",
    "# Content-transfer-encoding: Not enough data to add menaingful information - 7 bit appears to have a higher frequency with HAM email.\n",
    "# x-mailer: emails with x-mailer seem more likely to be HAM but this can be explored further in future iterations.\n",
    "\n",
    "dropping = ['mime-version', 'content-type', 'precedence', 'content-transfer-encoding', 'x-mailer']\n",
    "cleansed_email_df.drop(dropping, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8517 entries, 0 to 9349\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   index                  8517 non-null   object \n",
      " 1   target                 8517 non-null   object \n",
      " 2   from                   8517 non-null   object \n",
      " 3   to                     8517 non-null   object \n",
      " 4   subject                8517 non-null   object \n",
      " 5   body                   8517 non-null   object \n",
      " 6   to_fullname            2403 non-null   object \n",
      " 7   to_email               8517 non-null   object \n",
      " 8   to_email_count         8517 non-null   float64\n",
      " 9   to_username            8517 non-null   object \n",
      " 10  to_domain              8517 non-null   object \n",
      " 11  from_fullname          7213 non-null   object \n",
      " 12  from_email             8517 non-null   object \n",
      " 13  from_email_count       8517 non-null   float64\n",
      " 14  from_username          8517 non-null   object \n",
      " 15  from_domain            8517 non-null   object \n",
      " 16  content-type-format    7646 non-null   object \n",
      " 17  content-type-type      7646 non-null   object \n",
      " 18  content-type-charset   5359 non-null   object \n",
      " 19  content-type-encoding  1262 non-null   object \n",
      "dtypes: float64(2), object(18)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cleansed_email_df.info())\n",
    "df = deepcopy(cleansed_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_files():\n",
    "    url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\"\n",
    "    dl = requests.get(url, allow_redirects = True)\n",
    "    words = set(word.strip().lower() for word in str(dl.content).split(\"\\\\n\"))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_tags():\n",
    "    soup = BeautifulSoup(requests.get(\"https://www.w3schools.com/TAGS/default.ASP\").text)\n",
    "    table = soup.find('table')\n",
    "    trs = table.find_all('tr')\n",
    "    tags = [re.findall(r'<(.*)>', tr.find('td').get_text())[0] for tr in trs if tr.find('td') != None]\n",
    "    return (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_css_tags():\n",
    "    soup = BeautifulSoup(requests.get(\"https://www.w3schools.com/cssref/\").text)\n",
    "    tables = soup.find_all('table')\n",
    "    tables = [table.find_all('tr') for table in tables]\n",
    "    trs = [item for table in tables for item in table]\n",
    "    tags = [tr.find('td').get_text() for tr in trs]\n",
    "\n",
    "    return(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(df, valid_words):\n",
    "    \"\"\"\n",
    "    Function to encapsulate the process of generating a list of unique words from the body column.\n",
    "    Input:\n",
    "        df: target dataframe\n",
    "    Output:\n",
    "        unique_words: a list of all unique words contained in the body column of the target dataframe df.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[['index', 'body']]\n",
    "    punc_map = str.maketrans(dict.fromkeys(string.punctuation, ''))\n",
    "    df['words'] = df['body'].str.translate(punc_map).str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.split(\" \")\n",
    "    df.drop(['body'], axis = 1, inplace = True)\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    \n",
    "    # remove HTML tags - extracted from the W3 website\n",
    "    html_tags = get_html_tags()\n",
    "    \n",
    "    # remove HTML tags - extracted from the W3 website\n",
    "    css_tags = get_css_tags()\n",
    "    \n",
    "    # Common fonts\n",
    "    fonts = [\"Arial\", \"Verdana\", \"Helvetica\", \"Tahoma\", \"Trebuchet MS\", \"sans-serif\", \"sans serif\", \n",
    "             \"Times New Roman\", \"Georgia\", \"Garamond\", \"serif\", \"Courier New\", \n",
    "             \"monospace\", \"Brush Script MT\", \"cursive\"]\n",
    "    fonts = [font.lower() for font in fonts]\n",
    "    \n",
    "    words = filter(None, [a.strip().lower() for b in df['words'].tolist() for a in b])    \n",
    "    unique_words = set(words)\n",
    "\n",
    "\n",
    "#     unique_words_filtered = [word for word in unique_words if ('href' not in word and 'http' not in word)]\n",
    "#     unique_words_filtered = [word for word in unique_words_filtered if not word.isnumeric()]\n",
    "    \n",
    "#     d = enchant.Dict(\"en_US\")\n",
    "#     unique_words_us = [word for word in unique_words_filtered if d.check(word)]\n",
    "    \n",
    "#     d = enchant.Dict(\"en_GB\")\n",
    "#     unique_words_gb = [word for word in unique_words_filtered if d.check(word)]\n",
    "    \n",
    "    all_words = unique_words & valid_words - set(stopwords.words()) - set(html_tags) - set(css_tags) - set(fonts)\n",
    "    \n",
    "#     all_words = df[['body']].drop_duplicates()\n",
    "#     all_words['split'] = all_words['body'].astype(str).map(lambda x: x.split(\" \"))\n",
    "#     unique_words = (list(set([a.strip() for b in all_words['split'].tolist() for a in b])))\n",
    "    return (all_words, df[['index', 'words']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-1f42f2ba29f6>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['words'] = df['body'].str.translate(punc_map).str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.split(\" \")\n"
     ]
    }
   ],
   "source": [
    "unique_list_output = get_unique_words(df, get_words_files())\n",
    "df_new = unique_list_output[1]\n",
    "words = list(filter(None, [a.strip().lower() for b in df_new['words'].tolist() for a in b]))\n",
    "unique = unique_list_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28790"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_a = [word for word in words if word in unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(words_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 8673),\n",
       " ('email', 7486),\n",
       " ('list', 7250),\n",
       " ('new', 5450),\n",
       " ('get', 5203),\n",
       " ('sender', 5083),\n",
       " ('bulk', 4769),\n",
       " ('free', 4703),\n",
       " ('precedence', 4689),\n",
       " ('use', 4135),\n",
       " ('like', 3972),\n",
       " ('would', 3775),\n",
       " ('sep', 3761),\n",
       " ('message', 3588),\n",
       " ('dont', 3547),\n",
       " ('people', 3536),\n",
       " ('information', 3294),\n",
       " ('us', 3223),\n",
       " ('aug', 3199),\n",
       " ('mailing', 3112),\n",
       " ('please', 2997),\n",
       " ('make', 2994),\n",
       " ('web', 2994),\n",
       " ('group', 2718),\n",
       " ('users', 2667),\n",
       " ('may', 2572),\n",
       " ('wrote', 2547),\n",
       " ('business', 2524),\n",
       " ('see', 2350),\n",
       " ('money', 2331),\n",
       " ('found', 2324),\n",
       " ('normal', 2323),\n",
       " ('work', 2298),\n",
       " ('friends', 2298),\n",
       " ('know', 2279),\n",
       " ('way', 2275),\n",
       " ('could', 2248),\n",
       " ('internet', 2229),\n",
       " ('first', 2228),\n",
       " ('software', 2178),\n",
       " ('need', 2177),\n",
       " ('even', 2142),\n",
       " ('said', 2109),\n",
       " ('using', 2077),\n",
       " ('wed', 2039),\n",
       " ('system', 2025),\n",
       " ('oct', 2013),\n",
       " ('send', 1996),\n",
       " ('go', 1988),\n",
       " ('well', 1930),\n",
       " ('mail', 1928),\n",
       " ('sans', 1919),\n",
       " ('think', 1902),\n",
       " ('much', 1901),\n",
       " ('many', 1851),\n",
       " ('click', 1842),\n",
       " ('find', 1841),\n",
       " ('good', 1839),\n",
       " ('site', 1815),\n",
       " ('version', 1789),\n",
       " ('geneva', 1777),\n",
       " ('references', 1775),\n",
       " ('file', 1770),\n",
       " ('irish', 1768),\n",
       " ('world', 1728),\n",
       " ('help', 1708),\n",
       " ('receive', 1687),\n",
       " ('years', 1681),\n",
       " ('windows', 1656),\n",
       " ('spam', 1593),\n",
       " ('home', 1580),\n",
       " ('company', 1568),\n",
       " ('line', 1561),\n",
       " ('best', 1544),\n",
       " ('jul', 1494),\n",
       " ('service', 1486),\n",
       " ('clean', 1486),\n",
       " ('still', 1468),\n",
       " ('two', 1460),\n",
       " ('back', 1459),\n",
       " ('change', 1458),\n",
       " ('every', 1443),\n",
       " ('without', 1435),\n",
       " ('times', 1435),\n",
       " ('ive', 1429),\n",
       " ('subject', 1415),\n",
       " ('cc', 1407),\n",
       " ('really', 1394),\n",
       " ('online', 1388),\n",
       " ('last', 1382),\n",
       " ('used', 1369),\n",
       " ('rpm', 1365),\n",
       " ('report', 1361),\n",
       " ('day', 1356),\n",
       " ('phone', 1318),\n",
       " ('security', 1317),\n",
       " ('computer', 1317),\n",
       " ('since', 1299),\n",
       " ('news', 1289),\n",
       " ('set', 1288),\n",
       " ('fri', 1282),\n",
       " ('life', 1275),\n",
       " ('available', 1273),\n",
       " ('id', 1269),\n",
       " ('program', 1268),\n",
       " ('sent', 1267),\n",
       " ('discussion', 1257),\n",
       " ('today', 1250),\n",
       " ('thats', 1248),\n",
       " ('say', 1245),\n",
       " ('server', 1239),\n",
       " ('read', 1238),\n",
       " ('problem', 1210),\n",
       " ('files', 1208),\n",
       " ('something', 1208),\n",
       " ('number', 1179),\n",
       " ('next', 1168),\n",
       " ('try', 1163),\n",
       " ('year', 1153),\n",
       " ('better', 1152),\n",
       " ('sure', 1150),\n",
       " ('million', 1149),\n",
       " ('government', 1146),\n",
       " ('made', 1143),\n",
       " ('never', 1126),\n",
       " ('check', 1123),\n",
       " ('cant', 1121),\n",
       " ('build', 1121),\n",
       " ('support', 1117),\n",
       " ('another', 1112),\n",
       " ('companies', 1112),\n",
       " ('week', 1109),\n",
       " ('great', 1108),\n",
       " ('youre', 1099),\n",
       " ('look', 1092),\n",
       " ('within', 1090),\n",
       " ('outlook', 1090),\n",
       " ('old', 1083),\n",
       " ('must', 1080),\n",
       " ('going', 1074),\n",
       " ('search', 1072),\n",
       " ('removed', 1069),\n",
       " ('network', 1066),\n",
       " ('technology', 1055),\n",
       " ('things', 1047),\n",
       " ('start', 1046),\n",
       " ('long', 1044),\n",
       " ('doesnt', 1043),\n",
       " ('run', 1041),\n",
       " ('future', 1041),\n",
       " ('got', 1038),\n",
       " ('call', 1030),\n",
       " ('give', 1029),\n",
       " ('states', 1028),\n",
       " ('access', 1027),\n",
       " ('offer', 1023),\n",
       " ('messages', 1021),\n",
       " ('might', 1019),\n",
       " ('days', 1018),\n",
       " ('anyone', 1012),\n",
       " ('full', 1004),\n",
       " ('public', 1002),\n",
       " ('open', 1001),\n",
       " ('services', 990),\n",
       " ('product', 990),\n",
       " ('sun', 979),\n",
       " ('keep', 975),\n",
       " ('looking', 970),\n",
       " ('solid', 969),\n",
       " ('page', 968),\n",
       " ('management', 967),\n",
       " ('thanks', 964),\n",
       " ('real', 962),\n",
       " ('market', 959),\n",
       " ('received', 941),\n",
       " ('sponsored', 924),\n",
       " ('case', 914),\n",
       " ('following', 914),\n",
       " ('include', 912),\n",
       " ('wish', 911),\n",
       " ('different', 900),\n",
       " ('value', 896),\n",
       " ('remove', 890),\n",
       " ('lia', 888),\n",
       " ('price', 887),\n",
       " ('actually', 881),\n",
       " ('easy', 880),\n",
       " ('subscription', 879),\n",
       " ('text', 875),\n",
       " ('original', 871),\n",
       " ('put', 863),\n",
       " ('special', 862),\n",
       " ('less', 854),\n",
       " ('r', 852),\n",
       " ('instead', 847),\n",
       " ('united', 845),\n",
       " ('inc', 844),\n",
       " ('around', 842),\n",
       " ('part', 841),\n",
       " ('thing', 835),\n",
       " ('power', 833),\n",
       " ('maintainer', 832),\n",
       " ('however', 826),\n",
       " ('user', 824),\n",
       " ('state', 823),\n",
       " ('save', 823),\n",
       " ('point', 821),\n",
       " ('produced', 820),\n",
       " ('always', 817),\n",
       " ('else', 814),\n",
       " ('chris', 813),\n",
       " ('products', 810),\n",
       " ('visit', 810),\n",
       " ('credit', 808),\n",
       " ('none', 804),\n",
       " ('rights', 803),\n",
       " ('running', 800),\n",
       " ('working', 797),\n",
       " ('ever', 796),\n",
       " ('sites', 795),\n",
       " ('contact', 794),\n",
       " ('works', 793),\n",
       " ('card', 793),\n",
       " ('personal', 792),\n",
       " ('error', 790),\n",
       " ('someone', 784),\n",
       " ('copyright', 784),\n",
       " ('little', 781),\n",
       " ('systems', 778),\n",
       " ('perl', 777),\n",
       " ('probably', 776),\n",
       " ('marketing', 775),\n",
       " ('package', 773),\n",
       " ('though', 762),\n",
       " ('already', 757),\n",
       " ('tech', 757),\n",
       " ('least', 756),\n",
       " ('add', 753),\n",
       " ('including', 750),\n",
       " ('place', 747),\n",
       " ('begin', 744),\n",
       " ('based', 742),\n",
       " ('says', 740),\n",
       " ('theres', 740),\n",
       " ('red', 737),\n",
       " ('stuff', 734),\n",
       " ('able', 733),\n",
       " ('signature', 733),\n",
       " ('lot', 733),\n",
       " ('yes', 730),\n",
       " ('anything', 728),\n",
       " ('release', 727),\n",
       " ('reply', 726),\n",
       " ('size', 719),\n",
       " ('making', 718),\n",
       " ('problems', 718),\n",
       " ('issue', 716),\n",
       " ('seems', 709),\n",
       " ('copy', 706),\n",
       " ('f', 706),\n",
       " ('current', 703),\n",
       " ('media', 702),\n",
       " ('programs', 702),\n",
       " ('buy', 697),\n",
       " ('format', 695),\n",
       " ('getting', 694),\n",
       " ('account', 690),\n",
       " ('inline', 689),\n",
       " ('ms', 689),\n",
       " ('month', 688),\n",
       " ('etc', 687),\n",
       " ('yet', 685),\n",
       " ('enough', 684),\n",
       " ('course', 684),\n",
       " ('insurance', 683),\n",
       " ('done', 681),\n",
       " ('didnt', 678),\n",
       " ('simple', 678),\n",
       " ('legal', 678),\n",
       " ('high', 676),\n",
       " ('mr', 674),\n",
       " ('pay', 672),\n",
       " ('three', 671),\n",
       " ('hard', 670),\n",
       " ('addresses', 670),\n",
       " ('tell', 667),\n",
       " ('possible', 666),\n",
       " ('isnt', 665),\n",
       " ('simply', 664),\n",
       " ('express', 663),\n",
       " ('box', 660),\n",
       " ('job', 657),\n",
       " ('sat', 654),\n",
       " ('live', 653),\n",
       " ('key', 651),\n",
       " ('digital', 651),\n",
       " ('september', 648),\n",
       " ('months', 647),\n",
       " ('daily', 646),\n",
       " ('cost', 645),\n",
       " ('show', 643),\n",
       " ('financial', 642),\n",
       " ('complete', 642),\n",
       " ('example', 639),\n",
       " ('newsletter', 639),\n",
       " ('provide', 637),\n",
       " ('net', 629),\n",
       " ('trying', 628),\n",
       " ('others', 628),\n",
       " ('true', 627),\n",
       " ('pc', 622),\n",
       " ('seen', 621),\n",
       " ('info', 621),\n",
       " ('w', 620),\n",
       " ('offers', 620),\n",
       " ('let', 618),\n",
       " ('global', 617),\n",
       " ('large', 616),\n",
       " ('maybe', 614),\n",
       " ('groups', 613),\n",
       " ('american', 613),\n",
       " ('reported', 611),\n",
       " ('kernel', 610),\n",
       " ('type', 610),\n",
       " ('august', 609),\n",
       " ('fact', 609),\n",
       " ('via', 609),\n",
       " ('heaven', 608),\n",
       " ('yahoo', 606),\n",
       " ('july', 604),\n",
       " ('development', 603),\n",
       " ('organization', 603),\n",
       " ('weeks', 601),\n",
       " ('networks', 601),\n",
       " ('cash', 599),\n",
       " ('hours', 598),\n",
       " ('nothing', 598),\n",
       " ('cd', 596),\n",
       " ('either', 596),\n",
       " ('story', 591),\n",
       " ('experience', 590),\n",
       " ('links', 587),\n",
       " ('database', 586),\n",
       " ('install', 585),\n",
       " ('country', 584),\n",
       " ('office', 584),\n",
       " ('idea', 582),\n",
       " ('roman', 582),\n",
       " ('trade', 581),\n",
       " ('away', 580),\n",
       " ('question', 579),\n",
       " ('matthias', 579),\n",
       " ('application', 576),\n",
       " ('everything', 573),\n",
       " ('person', 573),\n",
       " ('law', 571),\n",
       " ('x', 570),\n",
       " ('pm', 569),\n",
       " ('welcome', 569),\n",
       " ('communications', 568),\n",
       " ('learn', 568),\n",
       " ('control', 568),\n",
       " ('important', 568),\n",
       " ('dollars', 567),\n",
       " ('importance', 566),\n",
       " ('talk', 564),\n",
       " ('grants', 564),\n",
       " ('makes', 560),\n",
       " ('several', 560),\n",
       " ('bit', 559),\n",
       " ('java', 558),\n",
       " ('return', 558),\n",
       " ('past', 556),\n",
       " ('stop', 556),\n",
       " ('called', 555),\n",
       " ('thought', 554),\n",
       " ('policy', 552),\n",
       " ('download', 551),\n",
       " ('features', 548),\n",
       " ('word', 548),\n",
       " ('plus', 547),\n",
       " ('reports', 547),\n",
       " ('write', 546),\n",
       " ('general', 546),\n",
       " ('gary', 546),\n",
       " ('added', 545),\n",
       " ('changes', 541),\n",
       " ('president', 540),\n",
       " ('interest', 539),\n",
       " ('process', 539),\n",
       " ('receiving', 537),\n",
       " ('ago', 536),\n",
       " ('play', 536),\n",
       " ('bad', 535),\n",
       " ('results', 535),\n",
       " ('lawrence', 533),\n",
       " ('log', 531),\n",
       " ('low', 531),\n",
       " ('allow', 530),\n",
       " ('family', 530),\n",
       " ('whether', 529),\n",
       " ('far', 527),\n",
       " ('become', 526),\n",
       " ('professional', 526),\n",
       " ('tm', 526),\n",
       " ('rather', 522),\n",
       " ('fax', 521),\n",
       " ('fast', 520),\n",
       " ('directory', 519),\n",
       " ('second', 518),\n",
       " ('fill', 517),\n",
       " ('investment', 517),\n",
       " ('often', 515),\n",
       " ('sell', 515),\n",
       " ('feel', 514),\n",
       " ('america', 513),\n",
       " ('mime', 512),\n",
       " ('amp', 512),\n",
       " ('note', 510),\n",
       " ('required', 509),\n",
       " ('apple', 508),\n",
       " ('bush', 508),\n",
       " ('drive', 507),\n",
       " ('sales', 507),\n",
       " ('lines', 504),\n",
       " ('quite', 502),\n",
       " ('rates', 502),\n",
       " ('believe', 500),\n",
       " ('folder', 498),\n",
       " ('update', 498),\n",
       " ('geek', 497),\n",
       " ('deal', 495),\n",
       " ('installed', 495),\n",
       " ('questions', 495),\n",
       " ('local', 490),\n",
       " ('opportunity', 489),\n",
       " ('latest', 488),\n",
       " ('domain', 485),\n",
       " ('ok', 484),\n",
       " ('murphy', 483),\n",
       " ('guide', 481),\n",
       " ('means', 480),\n",
       " ('wireless', 480),\n",
       " ('easily', 480),\n",
       " ('needs', 479),\n",
       " ('international', 479),\n",
       " ('create', 478),\n",
       " ('orders', 478),\n",
       " ('music', 475),\n",
       " ('kind', 475),\n",
       " ('income', 475),\n",
       " ('black', 474),\n",
       " ('currently', 474),\n",
       " ('sending', 473),\n",
       " ('given', 473),\n",
       " ('packages', 472),\n",
       " ('john', 470),\n",
       " ('razor', 470),\n",
       " ('supplied', 470),\n",
       " ('national', 468),\n",
       " ('act', 466),\n",
       " ('lists', 464),\n",
       " ('remember', 464),\n",
       " ('whole', 464),\n",
       " ('later', 463),\n",
       " ('mind', 463),\n",
       " ('cell', 463),\n",
       " ('reason', 463),\n",
       " ('love', 461),\n",
       " ('license', 461),\n",
       " ('industry', 460),\n",
       " ('popular', 460),\n",
       " ('test', 459),\n",
       " ('pretty', 458),\n",
       " ('reserved', 457),\n",
       " ('hit', 455),\n",
       " ('cannot', 455),\n",
       " ('book', 453),\n",
       " ('wont', 452),\n",
       " ('seem', 451),\n",
       " ('nice', 451),\n",
       " ('view', 450),\n",
       " ('names', 448),\n",
       " ('response', 448),\n",
       " ('hardware', 448),\n",
       " ('rate', 448),\n",
       " ('plan', 445),\n",
       " ('customers', 445),\n",
       " ('almost', 444),\n",
       " ('move', 443),\n",
       " ('post', 442),\n",
       " ('points', 442),\n",
       " ('soon', 441),\n",
       " ('upon', 441),\n",
       " ('single', 440),\n",
       " ('major', 439),\n",
       " ('alb', 439),\n",
       " ('worlds', 438),\n",
       " ('choice', 436),\n",
       " ('writes', 435),\n",
       " ('quick', 434),\n",
       " ('risk', 434),\n",
       " ('thousands', 434),\n",
       " ('join', 433),\n",
       " ('share', 433),\n",
       " ('edt', 433),\n",
       " ('worth', 431),\n",
       " ('tried', 431),\n",
       " ('included', 431),\n",
       " ('utc', 430),\n",
       " ('increase', 428),\n",
       " ('developers', 427),\n",
       " ('youve', 426),\n",
       " ('tools', 424),\n",
       " ('city', 424),\n",
       " ('federal', 423),\n",
       " ('electronic', 423),\n",
       " ('similar', 422),\n",
       " ('matter', 422),\n",
       " ('street', 421),\n",
       " ('wrong', 420),\n",
       " ('unseen', 419),\n",
       " ('pp', 419),\n",
       " ('includes', 417),\n",
       " ('preferences', 417),\n",
       " ('community', 417),\n",
       " ('mac', 415),\n",
       " ('fun', 414),\n",
       " ('level', 414),\n",
       " ('understand', 414),\n",
       " ('private', 414),\n",
       " ('mean', 413),\n",
       " ('window', 412),\n",
       " ('quality', 412),\n",
       " ('came', 411),\n",
       " ('whats', 410),\n",
       " ('servers', 409),\n",
       " ('client', 409),\n",
       " ('secure', 408),\n",
       " ('clients', 408),\n",
       " ('started', 408),\n",
       " ('contains', 407),\n",
       " ('minutes', 406),\n",
       " ('paid', 405),\n",
       " ('known', 404),\n",
       " ('tuesday', 404),\n",
       " ('comes', 403),\n",
       " ('habeas', 403),\n",
       " ('total', 403),\n",
       " ('bill', 402),\n",
       " ('games', 400),\n",
       " ('research', 399),\n",
       " ('needed', 397),\n",
       " ('political', 397),\n",
       " ('stock', 397),\n",
       " ('useful', 396),\n",
       " ('machine', 394),\n",
       " ('thank', 394),\n",
       " ('instructions', 394),\n",
       " ('gets', 393),\n",
       " ('operating', 393),\n",
       " ('standard', 393),\n",
       " ('prices', 393),\n",
       " ('words', 389),\n",
       " ('project', 389),\n",
       " ('interested', 388),\n",
       " ('applications', 387),\n",
       " ('purchase', 387),\n",
       " ('women', 386),\n",
       " ('coming', 386),\n",
       " ('tool', 385),\n",
       " ('letter', 385),\n",
       " ('ask', 384),\n",
       " ('hi', 383),\n",
       " ('recently', 383),\n",
       " ('bank', 383),\n",
       " ('told', 381),\n",
       " ('grant', 381),\n",
       " ('freedom', 379),\n",
       " ('due', 379),\n",
       " ('designed', 377),\n",
       " ('performance', 377),\n",
       " ('subscribed', 376),\n",
       " ('york', 375),\n",
       " ('quickly', 374),\n",
       " ('changed', 374),\n",
       " ('anyway', 374),\n",
       " ('mailings', 374),\n",
       " ('exactly', 374),\n",
       " ('looks', 373),\n",
       " ('lets', 371),\n",
       " ('memory', 371),\n",
       " ('customer', 371),\n",
       " ('human', 368),\n",
       " ('radio', 367),\n",
       " ('countries', 367),\n",
       " ('recent', 366),\n",
       " ('along', 365),\n",
       " ('building', 364),\n",
       " ('half', 364),\n",
       " ('result', 364),\n",
       " ('age', 363),\n",
       " ('advertising', 363),\n",
       " ('xp', 363),\n",
       " ('successful', 363),\n",
       " ('feature', 362),\n",
       " ('posted', 362),\n",
       " ('monday', 361),\n",
       " ('longer', 360),\n",
       " ('numbers', 359),\n",
       " ('method', 358),\n",
       " ('party', 358),\n",
       " ('growing', 358),\n",
       " ('garrigues', 356),\n",
       " ('hope', 356),\n",
       " ('protect', 355),\n",
       " ('listed', 355),\n",
       " ('ways', 353),\n",
       " ('step', 353),\n",
       " ('space', 352),\n",
       " ('everyone', 352),\n",
       " ('browser', 352),\n",
       " ('tired', 351),\n",
       " ('transaction', 351),\n",
       " ('hand', 350),\n",
       " ('friend', 350),\n",
       " ('although', 350),\n",
       " ('root', 350),\n",
       " ('regards', 349),\n",
       " ('goes', 349),\n",
       " ('answer', 349),\n",
       " ('gray', 349),\n",
       " ('trial', 349),\n",
       " ('apply', 349),\n",
       " ('force', 348),\n",
       " ('happy', 346),\n",
       " ('multipart', 346),\n",
       " ('review', 346),\n",
       " ('usa', 346),\n",
       " ('entire', 345),\n",
       " ('member', 345),\n",
       " ('five', 344),\n",
       " ('issues', 344),\n",
       " ('starting', 343),\n",
       " ('robert', 342),\n",
       " ('went', 342),\n",
       " ('fixed', 342),\n",
       " ('forward', 342),\n",
       " ('white', 341),\n",
       " ('sort', 341),\n",
       " ('amount', 340),\n",
       " ('guaranteed', 340),\n",
       " ('takes', 339),\n",
       " ('ideas', 339),\n",
       " ('follow', 339),\n",
       " ('hundreds', 339),\n",
       " ('comments', 337),\n",
       " ('absolutely', 337),\n",
       " ('cause', 337),\n",
       " ('together', 336),\n",
       " ('3d', 336),\n",
       " ('wanted', 335),\n",
       " ('living', 335),\n",
       " ('four', 335),\n",
       " ('request', 335),\n",
       " ('registered', 335),\n",
       " ('provided', 335),\n",
       " ('common', 335),\n",
       " ('economic', 334),\n",
       " ('billion', 334),\n",
       " ('theyre', 332),\n",
       " ('capital', 331),\n",
       " ('provides', 331),\n",
       " ('press', 330),\n",
       " ('comment', 330),\n",
       " ('ability', 330),\n",
       " ('perhaps', 330),\n",
       " ('turn', 330),\n",
       " ('close', 330),\n",
       " ('asked', 329),\n",
       " ('interesting', 328),\n",
       " ('continue', 328),\n",
       " ('pages', 328),\n",
       " ('immediately', 327),\n",
       " ('paper', 327),\n",
       " ('professionals', 327),\n",
       " ('limited', 326),\n",
       " ('success', 326),\n",
       " ('havent', 325),\n",
       " ('potential', 325),\n",
       " ('various', 325),\n",
       " ('image', 325),\n",
       " ('reading', 324),\n",
       " ('enter', 324),\n",
       " ('lose', 324),\n",
       " ('health', 323),\n",
       " ('notice', 323),\n",
       " ('partners', 322),\n",
       " ('allows', 322),\n",
       " ('computers', 322),\n",
       " ('game', 321),\n",
       " ('property', 320),\n",
       " ('exchange', 320),\n",
       " ('justin', 319),\n",
       " ('device', 318),\n",
       " ('interactive', 318),\n",
       " ('cards', 318),\n",
       " ('reach', 317),\n",
       " ('across', 316),\n",
       " ('according', 316),\n",
       " ('design', 315),\n",
       " ('privacy', 315),\n",
       " ('default', 314),\n",
       " ('couple', 314),\n",
       " ('fall', 314),\n",
       " ('lost', 313),\n",
       " ('register', 313),\n",
       " ('heard', 312),\n",
       " ('heres', 312),\n",
       " ('house', 312),\n",
       " ('writing', 311),\n",
       " ('tips', 310),\n",
       " ('bug', 309),\n",
       " ('school', 309),\n",
       " ('team', 309),\n",
       " ('foreign', 309),\n",
       " ('especially', 308),\n",
       " ('short', 308),\n",
       " ('taken', 307),\n",
       " ('interface', 307),\n",
       " ('fine', 307),\n",
       " ('car', 307),\n",
       " ('load', 306),\n",
       " ('secret', 306),\n",
       " ('happen', 306),\n",
       " ('action', 306),\n",
       " ('direct', 305),\n",
       " ('technologies', 305),\n",
       " ('solution', 305),\n",
       " ('print', 305),\n",
       " ('shipping', 305),\n",
       " ('james', 305),\n",
       " ('average', 305),\n",
       " ('ready', 303),\n",
       " ('collection', 303),\n",
       " ('decided', 303),\n",
       " ('dr', 303),\n",
       " ('enjoy', 302),\n",
       " ('powerful', 302),\n",
       " ('members', 301),\n",
       " ('store', 300),\n",
       " ('computing', 299),\n",
       " ('trust', 298),\n",
       " ('broadcast', 298),\n",
       " ('social', 298),\n",
       " ('assistance', 298),\n",
       " ('claim', 297),\n",
       " ('command', 296),\n",
       " ('options', 296),\n",
       " ('saying', 296),\n",
       " ('membership', 296),\n",
       " ('uses', 295),\n",
       " ('dave', 295),\n",
       " ('hear', 295),\n",
       " ('ones', 295),\n",
       " ('completely', 294),\n",
       " ('win', 294),\n",
       " ('late', 294),\n",
       " ('bread', 294),\n",
       " ('french', 293),\n",
       " ('fastest', 293),\n",
       " ('opportunities', 293),\n",
       " ('gif', 293),\n",
       " ('specific', 292),\n",
       " ('multiple', 292),\n",
       " ('advantage', 291),\n",
       " ('created', 290),\n",
       " ('term', 290),\n",
       " ('talking', 290),\n",
       " ('directly', 289),\n",
       " ('taking', 289),\n",
       " ('journal', 289),\n",
       " ('removal', 289),\n",
       " ('fork', 287),\n",
       " ('took', 287),\n",
       " ('bring', 287),\n",
       " ('october', 286),\n",
       " ('manager', 286),\n",
       " ('worked', 285),\n",
       " ('side', 285),\n",
       " ('thinking', 284),\n",
       " ('newsletters', 284),\n",
       " ('unique', 283),\n",
       " ('wouldnt', 283),\n",
       " ('platform', 283),\n",
       " ('weight', 283),\n",
       " ('invoked', 282),\n",
       " ('language', 282),\n",
       " ('guarantee', 282),\n",
       " ('selling', 281),\n",
       " ('function', 281),\n",
       " ('military', 281),\n",
       " ('aol', 281),\n",
       " ('additional', 281),\n",
       " ('huge', 281),\n",
       " ('fix', 280),\n",
       " ('finally', 280),\n",
       " ('temple', 279),\n",
       " ('university', 278),\n",
       " ('written', 278),\n",
       " ('tv', 278),\n",
       " ('ie', 278),\n",
       " ('folks', 278),\n",
       " ('oh', 278),\n",
       " ('rest', 278),\n",
       " ('desktop', 277),\n",
       " ('benefit', 276),\n",
       " ('finding', 276),\n",
       " ('cheers', 275),\n",
       " ('likely', 275),\n",
       " ('terms', 275),\n",
       " ('corporation', 275),\n",
       " ('player', 275),\n",
       " ('sense', 274),\n",
       " ('nations', 274),\n",
       " ('environment', 273),\n",
       " ('screen', 273),\n",
       " ('existing', 273),\n",
       " ('status', 273),\n",
       " ('automatically', 273),\n",
       " ('rules', 272),\n",
       " ('upgrade', 272),\n",
       " ('sound', 272),\n",
       " ('trading', 272),\n",
       " ('signed', 271),\n",
       " ('wasnt', 271),\n",
       " ('communication', 271),\n",
       " ('gives', 271),\n",
       " ('sign', 271),\n",
       " ('particular', 271),\n",
       " ('history', 270),\n",
       " ('effective', 270),\n",
       " ('events', 270),\n",
       " ('processing', 270),\n",
       " ('jobs', 270),\n",
       " ('record', 269),\n",
       " ('traffic', 269),\n",
       " ('david', 269),\n",
       " ('certainly', 268),\n",
       " ('connection', 268),\n",
       " ('leads', 268),\n",
       " ('basic', 268),\n",
       " ('weve', 267),\n",
       " ('growth', 267),\n",
       " ('energy', 267),\n",
       " ('released', 266),\n",
       " ('fcc', 266),\n",
       " ('whatever', 265),\n",
       " ('editor', 264),\n",
       " ('accept', 264),\n",
       " ('banner', 264),\n",
       " ('tax', 264),\n",
       " ('wants', 263),\n",
       " ('column', 263),\n",
       " ('comic', 263),\n",
       " ('storage', 263),\n",
       " ('pick', 262),\n",
       " ('guess', 262),\n",
       " ('millions', 262),\n",
       " ('pdt', 262),\n",
       " ('knowledge', 262),\n",
       " ('sex', 261),\n",
       " ('unless', 261),\n",
       " ('mortgage', 261),\n",
       " ('wait', 260),\n",
       " ('setup', 260),\n",
       " ('loan', 259),\n",
       " ('fully', 259),\n",
       " ('court', 259),\n",
       " ('adam', 259),\n",
       " ('dear', 259),\n",
       " ('choose', 258),\n",
       " ('involved', 258),\n",
       " ('michael', 257),\n",
       " ('advertisement', 257),\n",
       " ('resources', 257),\n",
       " ('effort', 256),\n",
       " ('third', 256),\n",
       " ('shows', 256),\n",
       " ('lots', 255),\n",
       " ('leave', 255),\n",
       " ('related', 255),\n",
       " ('air', 255),\n",
       " ('speed', 255),\n",
       " ('feb', 255),\n",
       " ('sponsor', 254),\n",
       " ('commercial', 254),\n",
       " ('bold', 254),\n",
       " ('lives', 254),\n",
       " ('sometimes', 253),\n",
       " ('early', 253),\n",
       " ('disk', 253),\n",
       " ('quote', 253),\n",
       " ('bonus', 253),\n",
       " ('easier', 252),\n",
       " ('transfer', 252),\n",
       " ('respect', 252),\n",
       " ('cool', 252),\n",
       " ('unix', 251),\n",
       " ('among', 251),\n",
       " ('six', 251),\n",
       " ('anywhere', 250),\n",
       " ('commission', 250),\n",
       " ('cases', 250),\n",
       " ('washington', 249),\n",
       " ('necessary', 248),\n",
       " ('require', 248),\n",
       " ('youd', 248),\n",
       " ('fund', 248),\n",
       " ('canon', 248),\n",
       " ('array', 247),\n",
       " ('director', 247),\n",
       " ('agree', 247),\n",
       " ('individual', 247),\n",
       " ('expect', 246),\n",
       " ('except', 245),\n",
       " ('giving', 245),\n",
       " ('certain', 245),\n",
       " ('hour', 245),\n",
       " ('willing', 245),\n",
       " ('valuable', 245),\n",
       " ('engines', 244),\n",
       " ('tom', 244),\n",
       " ('intended', 244),\n",
       " ('china', 244),\n",
       " ('rule', 243),\n",
       " ('percent', 243),\n",
       " ('uk', 243),\n",
       " ('plans', 243),\n",
       " ('department', 243),\n",
       " ('moment', 242),\n",
       " ('books', 242),\n",
       " ('bits', 242),\n",
       " ('appears', 241),\n",
       " ('usually', 241),\n",
       " ('investors', 241),\n",
       " ('blue', 240),\n",
       " ('versions', 240),\n",
       " ('leading', 240),\n",
       " ('cds', 240),\n",
       " ('chance', 239),\n",
       " ('extra', 239),\n",
       " ('faster', 239),\n",
       " ('avoid', 239),\n",
       " ('meet', 239),\n",
       " ('behind', 238),\n",
       " ('procedure', 238),\n",
       " ('g', 238),\n",
       " ('situation', 238),\n",
       " ('model', 238),\n",
       " ('wall', 238),\n",
       " ('administration', 238),\n",
       " ('oz', 238),\n",
       " ('built', 237),\n",
       " ('apt', 237),\n",
       " ('arent', 236),\n",
       " ('thus', 236),\n",
       " ('charge', 235),\n",
       " ('edition', 235),\n",
       " ('submit', 235),\n",
       " ('cut', 234),\n",
       " ('requires', 234),\n",
       " ('lowest', 234),\n",
       " ('series', 233),\n",
       " ('patch', 233),\n",
       " ('executive', 232),\n",
       " ('near', 231),\n",
       " ('echo', 231),\n",
       " ('rss', 231),\n",
       " ('backup', 231),\n",
       " ('sorry', 230),\n",
       " ('tip', 230),\n",
       " ('sequences', 229),\n",
       " ('former', 229),\n",
       " ('strategy', 229),\n",
       " ('suite', 228),\n",
       " ('conference', 228),\n",
       " ('board', 228),\n",
       " ('block', 228),\n",
       " ('loss', 228),\n",
       " ('corporate', 228),\n",
       " ('congress', 227),\n",
       " ('gpl', 227),\n",
       " ('improvement', 227),\n",
       " ('calls', 227),\n",
       " ('natural', 227),\n",
       " ('held', 226),\n",
       " ('society', 226),\n",
       " ('difficult', 226),\n",
       " ('imo', 226),\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_map(words, row):\n",
    "    \"\"\"\n",
    "    Passing in a list of unique cuisines that has been generated previously.\n",
    "    We compare the cuisines in a particular row and return a binary list of 0 for False, 1 for True in the cuisines is\n",
    "    in the row values.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = [word for word in row if word in words]\n",
    "    word_counts = Counter(word_list)\n",
    "    \n",
    "    \n",
    "    return (word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words_df(df):\n",
    "    df_full = pd.DataFrame(df)\n",
    "    df_full = df_full[df_full['body'].notna()]\n",
    "    \n",
    "    try:\n",
    "        df_full.reset_index(inplace = True)\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        print(\"Index not reset\")\n",
    "    \n",
    "    mapping_dict = {}\n",
    "    valid_words = get_words_files()\n",
    "    words_mapping = get_unique_words(df_full, valid_words)\n",
    "    \n",
    "    unique_words = words_mapping[0]\n",
    "    words_df = words_mapping[1]\n",
    "    \n",
    "    for i in tqdm(range(words_df.shape[0])):\n",
    "        word_map = get_word_map(unique_words, words_df['words'][i])\n",
    "        mapping_dict[words_df['index'][i]] = word_map\n",
    "    word_mapping_df = pd.DataFrame.from_dict(mapping_dict, orient = 'index', columns = unique_words)\n",
    "    return word_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-230-8cd17751a38a>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['words'] = df['body'].str.translate(punc_map).str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.split(\" \")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 8517/8517 [00:01<00:00, 7264.48it/s]\n"
     ]
    }
   ],
   "source": [
    "parsed = parse_words_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.iloc[0:100, 0:100].to_csv(\"./parsed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nn for embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
