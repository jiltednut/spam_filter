{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Liam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Liam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import bz2\n",
    "import tarfile\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# Language processing\n",
    "import string\n",
    "\n",
    "import enchant\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# URL libs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to specify where to get the email data (lib_url) and where to save it (main_dir, sub_dir)\n",
    "# These will be referenced in the data curation phase of execution.\n",
    "lib_url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "main_dir = \"data\"\n",
    "sub_dir = \"extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure(main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to create the directory structure on disk in the case that it doesn't already exist\n",
    "    The directory will be created in the same directory as the source file and will use the structure main_dir/sub_dir\n",
    "    Input:\n",
    "        main_dir: the top level of the directory structure\n",
    "        sub_dir: sublevel in the directory structure\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(\".\\\\\" + main_dir)\n",
    "    except:\n",
    "        print(\"Directory already exists.\")\n",
    "        try:\n",
    "            os.mkdir(\".\\\\\"+ main_dir +\"\\\\\"+ sub_dir +\"\\\\\")\n",
    "        except:\n",
    "            print(\"Directory already exists.\")\n",
    "    print(\"Directory Structure Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_email_records(url):\n",
    "    \"\"\"\n",
    "    Function to download the page source from online directory and create a list of filenames with .tar.bz2 extensions\n",
    "    Input:\n",
    "        url: the lib_url defined in the source\n",
    "    Returns:\n",
    "        downloadable: a list of urls made of the lib_url web address and the filenames extracted from the hyperlines in the page source.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(requests.get(lib_url).text)\n",
    "    urls = soup.find_all('a')\n",
    "    filenames = [url['href'] for url in urls if \"bz2\" in str(url)]\n",
    "    downloadable = [lib_url + filename for filename in filenames]\n",
    "    print(\"Email archive urls extracted\")\n",
    "    return downloadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_email_records(file_urls):\n",
    "    \"\"\"\n",
    "    Function to download the email archives and write them to disk in the directory hierarchy\n",
    "    Input:\n",
    "        file_urls: a list of urls pointing to the email archives\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    for i, url in enumerate(file_urls):\n",
    "        dl = requests.get(url, allow_redirects = True)\n",
    "        open(\".//\" + main_dir + \"//\"+file_urls[i].split(\"/\")[-1], 'wb').write(dl.content)\n",
    "    print(\"Email archives downloaded from url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_records(main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to extract the email records from the downloaded email archives.\n",
    "    Loops through all files in each level of the directory hierarchy and extracts the data from all tar.bz2 archives to disk.\n",
    "    Input:\n",
    "        main_dir: the top level of the directory structure\n",
    "        sub_dir: sublevel in the directory structure\n",
    "    Returns:\n",
    "        No return.\n",
    "    \"\"\"\n",
    "    for filepath in glob.glob(\".\\\\\" + main_dir + \"\\\\*.tar.bz2\"):\n",
    "        #zipfile = bz2.BZ2File(filepath)\n",
    "        #data = zipfile.read()\n",
    "        #newfile = filepath[:-4]\n",
    "        #open(newfile, \"wb\").write(data)\n",
    "        tar = tarfile.open(filepath, \"r:bz2\")\n",
    "        tar.extractall(os.path.join(main_dir+ \"\\\\\"+ sub_dir, filepath[7:-8]))\n",
    "        tar.close()\n",
    "    print(\"Email records extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_and_create_email_records(url, main_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Wrapper function to download and create the email records from the archive.\n",
    "    \"\"\"\n",
    "    urls = download_email_records(url)\n",
    "    create_directory_structure(main_dir = main_dir, sub_dir = sub_dir)\n",
    "    save_email_records(file_urls = urls)\n",
    "    extract_email_records(main_dir = main_dir, sub_dir = sub_dir)\n",
    "    print(\"Email records downloaded & extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_directory_details(target_dir, sub_dir):\n",
    "    \"\"\"\n",
    "    Function to traverse a directory and record the target type of each folder by checking if the folder\n",
    "    contains either \"HAM\" or \"SPAM\" in the name.\n",
    "    Input:\n",
    "        target_dir: the target directory\n",
    "        sub_dir: the sub directory\n",
    "    Returns:\n",
    "        email_type_names: a list containing the folder path, in the sub_dir, and the target type based on the folder name.\n",
    "    \"\"\"\n",
    "    sub_directories = glob.glob(target_dir + \"\\\\\"+ sub_dir +\"\\\\*\\\\*\")\n",
    "#     print(target_dir)\n",
    "#     print(os.path.join(target_dir, \"\\\\extracted\\\\*\\\\*\"))\n",
    "#     print(sub_directories)\n",
    "    names = [(x.split(\"\\\\\")[-1], \"HAM\" if x.find(\"ham\") >=0 else \"SPAM\") for x in sub_directories]\n",
    "    email_type_names = list(zip(names, sub_directories))\n",
    "    print(\"Target directories extracted\")\n",
    "    return email_type_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(email, line_names, target):\n",
    "    \"\"\"\n",
    "    Function to take in the filename of an email document.\n",
    "    Extract any information relating to the predefined tags\n",
    "    Store any information after the subject line as body - to be further processed later\n",
    "    Input:\n",
    "        email: the email text file, extracted from the email archive\n",
    "        line_names: a predefined list of line start strings that will correspond to column headers later\n",
    "        target: the target type extracted from the home folder of the email file.\n",
    "    Returns:\n",
    "        value_dict: a dictionary with the extracted body text, target type and key-value pairs for the line_names values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(email) as file:\n",
    "            body_start = False # Changed to True after reading the subject tag.\n",
    "            body = []\n",
    "            value_dict = {}\n",
    "            value_dict['target'] = target\n",
    "\n",
    "            for line in file.readlines():\n",
    "                line_start = line.split(\":\")[0]+\":\"\n",
    "                if body_start:\n",
    "                    body.append(line.strip())\n",
    "                if line_start in line_names:\n",
    "                    line_contents = re.findall(r\":\\s(.*)\", line)[0]\n",
    "                    value_dict[line_start] = line_contents\n",
    "                if line_start == \"Subject:\":\n",
    "                    body_start = True\n",
    "            value_dict['body'] = \"\\n\".join(body)\n",
    "            return value_dict\n",
    "    except Exception as e:\n",
    "        print(f\"{e}: Error: Can't read file {email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_target_mappings(main_dir):\n",
    "    \"\"\"\n",
    "    Function to map the target type to the folder name.\n",
    "    Used to map the target to the individual email text files later.\n",
    "    Input:\n",
    "        main_dir: The directory that needs to be mapped\n",
    "    Returns:\n",
    "        target_mapping: list of tuples containing the folder path & the target type (\"HAM\" or \"SPAM\").    \n",
    "    \"\"\"\n",
    "    email_type_names = get_target_directory_details(\".\\\\\" + main_dir, sub_dir)\n",
    "    directories = [x[1] for x in email_type_names]\n",
    "    targets = [x[0][1] for x in email_type_names]\n",
    "    target_mapping = list(zip(directories, targets))\n",
    "    print(\"Target mappings extracted\")\n",
    "    return target_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_file_listing(dir_path):\n",
    "    \"\"\"\n",
    "    Function to create a list of all the file paths in a directory\n",
    "    Input:\n",
    "        dir_path: the file path of a directory\n",
    "    Returns:\n",
    "        a list of all files in the directory.\n",
    "    \"\"\"\n",
    "    print(dir_path + \"\\\\\")\n",
    "    return glob.glob(dir_path + \"\\\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_data_to_dictionary(main_dir):\n",
    "    \"\"\"\n",
    "    Function to extract the details from the email text files and store in a dictionary\n",
    "    Inputs:\n",
    "        main_dir: the directory containing the email text files\n",
    "    Returns:\n",
    "        email_contents: dictionary containing the extracted dictionaries from the function parse_email().\n",
    "    \"\"\"\n",
    "    line_names = [\"To:\", \"From:\", \"MIME-Version:\", \"Content-Type:\",\n",
    "                 \"Content-Transfer-Encoding:\", \"X-Mailer:\", \"Subject:\",\n",
    "                 \"Precedence:\"]\n",
    "\n",
    "    target_mapping = get_email_target_mappings(main_dir = main_dir)\n",
    "    email_contents = {}\n",
    "    for target in target_mapping:\n",
    "        for file in get_directory_file_listing(target[0]):\n",
    "            email_contents[file.split(\"\\\\\")[-1]] = parse_email(file, line_names, target[1])\n",
    "    print(\"Emails extracted to dictionary\")\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dataframe(email_dict):\n",
    "    \"\"\"\n",
    "    Function to convert a dictionary to a dataframe and tranpose the resulting dataframe.\n",
    "    Input:\n",
    "        email_dict: a dictionary containing dictionaries with extracted email information\n",
    "    Returns:\n",
    "        df: dataframe generated from the dictionary, transposed to keep keys as the columns and not as the rows.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_dict(email_dict).transpose().reset_index()\n",
    "    print(\"DataFrame generated\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_email_dataframe():\n",
    "    \"\"\"\n",
    "    Wrapper function to return a dataframe from the extracted email archives\n",
    "    \"\"\"\n",
    "    email_dict = extract_email_data_to_dictionary(main_dir = main_dir)\n",
    "    print(\"Base dataframe ready for cleansing\")\n",
    "    return convert_dict_to_dataframe(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target directories extracted\n",
      "Target mappings extracted\n",
      ".\\data\\extracted\\20021010_easy_ham\\easy_ham\\\n",
      ".\\data\\extracted\\20021010_hard_ham\\hard_ham\\\n",
      ".\\data\\extracted\\20021010_spam\\spam\\\n",
      "'charmap' codec can't decode byte 0x81 in position 3082: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0123.68e87f8b736959b1ab5c4b5f2ce7484a\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0255.42a6feb4435a0a68929075c0926f085d\n",
      "'charmap' codec can't decode byte 0x81 in position 2588: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0273.51c482172b47ce926021aa7cc2552549\n",
      "'charmap' codec can't decode byte 0x81 in position 2503: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0330.a4df526233e524104c3b3554dd8ab5a8\n",
      "'charmap' codec can't decode byte 0x81 in position 2682: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0334.3e4946e69031f3860ac6de3d3f27aadd\n",
      "'charmap' codec can't decode byte 0x81 in position 2728: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20021010_spam\\spam\\0335.9822e1787fca0741a8501bdef7e8bc79\n",
      ".\\data\\extracted\\20030228_easy_ham\\easy_ham\\\n",
      ".\\data\\extracted\\20030228_easy_ham_2\\easy_ham_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_easy_ham_2\\easy_ham_2\\00664.28f4cb9fad800d0c7175d3a67e6c6458\n",
      ".\\data\\extracted\\20030228_hard_ham\\hard_ham\\\n",
      ".\\data\\extracted\\20030228_spam\\spam\\\n",
      "'charmap' codec can't decode byte 0x81 in position 3124: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00116.29e39a0064e2714681726ac28ff3fdef\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00245.f129d5e7df2eebd03948bb4f33fa7107\n",
      "'charmap' codec can't decode byte 0x81 in position 2568: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00263.13fc73e09ae15e0023bdb13d0a010f2d\n",
      "'charmap' codec can't decode byte 0x81 in position 2483: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00320.20dcbb5b047b8e2f212ee78267ee27ad\n",
      "'charmap' codec can't decode byte 0x81 in position 2662: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00323.9e36bf05304c99f2133a4c03c49533a9\n",
      "'charmap' codec can't decode byte 0x81 in position 2708: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00324.6f320a8c6b5f8e4bc47d475b3d4e86ef\n",
      "'charmap' codec can't decode byte 0x8f in position 1674: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam\\spam\\00500.85b72f09f6778a085dc8b6821965a76f\n",
      ".\\data\\extracted\\20030228_spam_2\\spam_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\00721.09d243c9c4da88c5f517003d26196aaa\n",
      "'charmap' codec can't decode byte 0x8d in position 3062: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01065.9ecef01b01ca912fa35453196b4dae4c\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01083.a6b3c50be5abf782b585995d2c11176b\n",
      "'charmap' codec can't decode byte 0x90 in position 2832: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01227.04a4f94c7a73b29cb56bf38c7d526116\n",
      "'charmap' codec can't decode byte 0x9d in position 4099: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20030228_spam_2\\spam_2\\01376.73e738e4cd8121ce3dfb42d190b193c9\n",
      ".\\data\\extracted\\20050311_spam_2\\spam_2\\\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\00721.09d243c9c4da88c5f517003d26196aaa\n",
      "'charmap' codec can't decode byte 0x8d in position 3062: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01065.9ecef01b01ca912fa35453196b4dae4c\n",
      "list index out of range: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01083.a6b3c50be5abf782b585995d2c11176b\n",
      "'charmap' codec can't decode byte 0x90 in position 2832: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01227.04a4f94c7a73b29cb56bf38c7d526116\n",
      "'charmap' codec can't decode byte 0x9d in position 4099: character maps to <undefined>: Error: Can't read file .\\data\\extracted\\20050311_spam_2\\spam_2\\01376.73e738e4cd8121ce3dfb42d190b193c9\n",
      "Emails extracted to dictionary\n",
      "Base dataframe ready for cleansing\n",
      "DataFrame generated\n"
     ]
    }
   ],
   "source": [
    "#dl_and_create_email_records(lib_url, main_dir, sub_dir)\n",
    "base_email_df = generate_base_email_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_email_df.to_parquet(\"base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_email = pd.read_parquet(\"base.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed_email_df = deepcopy(base_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9350 entries, 0 to 9349\n",
      "Data columns (total 11 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   index                       9350 non-null   object\n",
      " 1   target                      9331 non-null   object\n",
      " 2   From:                       9329 non-null   object\n",
      " 3   To:                         9006 non-null   object\n",
      " 4   Subject:                    9322 non-null   object\n",
      " 5   MIME-Version:               6208 non-null   object\n",
      " 6   Content-Type:               8052 non-null   object\n",
      " 7   Precedence:                 5304 non-null   object\n",
      " 8   body                        9331 non-null   object\n",
      " 9   X-Mailer:                   3650 non-null   object\n",
      " 10  Content-Transfer-Encoding:  4604 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 803.6+ KB\n"
     ]
    }
   ],
   "source": [
    "cleansed_email_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_remove_colon_from_column_name(df):\n",
    "    \"\"\"\n",
    "    Function to remove the colon from the column headers and force the text to lowercase\n",
    "    Input:\n",
    "        df: the target dataframe\n",
    "    Returns:\n",
    "        df: df with renamed columns\n",
    "    \"\"\"\n",
    "    df.columns = [x.replace(\":\", \"\").lower() for x in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_components_to_features(df, user_types):\n",
    "    \"\"\"\n",
    "    Function to extract components of the to & from columns to new features\n",
    "        fullname: the fullname of the sender that prefixs the email address\n",
    "        email: the full email address contained in '<email_address>'\n",
    "        username: the username from the email address (everything before @)\n",
    "        domain: the domain of the email address (everything after @)\n",
    "    Inputs:\n",
    "        df: the target dataframe\n",
    "        user_types: list of types of user that will be processed i.e. ['to'], ['from'], ['to', 'from']\n",
    "    Returns:\n",
    "        df: df with additional features added.\n",
    "    \"\"\"\n",
    "    for user_type in user_types:\n",
    "        # Split the FROM column into full name, username and domain\n",
    "        # df[str(user_type + '_fullname')] = df[str(user_type)].str.split(\"<\", n = 1).str[0].str.replace('\"', \"\")\n",
    "        df[str(user_type + '_fullname')] = df[str(user_type)].str.extract(r'[$\\s\\\"]?([\\w\\d\\s]*)[\\s\\\"]')[0]\n",
    "\n",
    "        # Extract the from email\n",
    "        # df[str(user_type + '_email')] = df[str(user_type)].str.split(\"<\").str[1].str.replace('>', \"\")\n",
    "        # df[str(user_type + '_email')] = df[str(user_type)].str.extract(r'[\\s<]?([\\w\\d\\+]*@.*\\.[\\w\\d]*)')[0]\n",
    "        df[str(user_type + '_email')] = df[str(user_type)].str.extract(r'([\\w\\d\\+]+@[\\w\\d]+\\.[\\w\\d]+)')[0]\n",
    "        df[str(user_type + '_email_count')] = df[str(user_type)].str.count(r'([\\w\\d\\+]+@[\\w\\d]+\\.[\\w\\d]+)')\n",
    "        \n",
    "        # Extract the from username\n",
    "        df[str(user_type + '_username')] = df[str(user_type + '_email')].str.extract(r'(.*)[@]')\n",
    "\n",
    "        # Extract the from domain\n",
    "        df[str(user_type + '_domain')] = df[str(user_type + '_email')].str.extract(r'[@](.*)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_invalid_to_from_subject_target_records(df):\n",
    "    \"\"\"\n",
    "    Function to exclude records with invalid target, to, from & subject.\n",
    "    Input:\n",
    "        df: email contents dataframe\n",
    "    Returns:\n",
    "        df: email contents dataframe without invaid target, to, from & subject rows.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df[df['target'].notna()]\n",
    "    df = df[df['to'].notna()]\n",
    "    df = df[df['from'].notna()]\n",
    "    df = df[df['subject'].notna()]\n",
    "    df = df[df['to_email'].notna()]\n",
    "    df = df[df['from_email'].notna()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_type_info_from_content_type_records(df):\n",
    "    \"\"\"\n",
    "    Function to extract format, type, encoding & character set information from the content-type string\n",
    "    Input:\n",
    "        df: email contents dataframe\n",
    "    Returns:\n",
    "        df: email contents dataframe with additional columns for content-type data\n",
    "    \n",
    "    \"\"\"\n",
    "    df['content-type-format'] = df['content-type'].str.lower().str.extract(r'^(\\w+)/')\n",
    "    df['content-type-type'] = df['content-type'].str.lower().str.extract(r'^\\w+/(\\w+)[;\\s]?')\n",
    "    df['content-type-charset'] = df['content-type'].str.lower().str.extract(r'charset[\\s]?=[\\\"]?([\\w\\d-]+)[\\\"\\s]?')\n",
    "    df['content-type-encoding'] = df['content-type'].str.lower().str.extract(r'encoding[\\s]?=[\\\"]?([\\w\\d-]+)[\\\"\\s]?')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try functions\n",
    "cleansed_email_df = deepcopy(base_email_df)\n",
    "cleansed_email_df = rename_columns_remove_colon_from_column_name(cleansed_email_df)\n",
    "cleansed_email_df = extract_email_components_to_features(cleansed_email_df, ['to', 'from'])\n",
    "cleansed_email_df = exclude_invalid_to_from_subject_target_records(cleansed_email_df)\n",
    "cleansed_email_df = extract_content_type_info_from_content_type_records(cleansed_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop after review\n",
    "# mime-version: no appreciable relevance - all values are 1 with an insignificant qty including additional info (approx 2%)\n",
    "# content-type: feature extraction is complete\n",
    "# Precedence: no appreciable relevance - no alignment between bulk and to_email_count and no obvious way to infer type. Possibly revisit or attempt to create a feature independently\n",
    "# Content-transfer-encoding: Not enough data to add menaingful information - 7 bit appears to have a higher frequency with HAM email.\n",
    "# x-mailer: emails with x-mailer seem more likely to be HAM but this can be explored further in future iterations.\n",
    "\n",
    "dropping = ['mime-version', 'content-type', 'precedence', 'content-transfer-encoding', 'x-mailer']\n",
    "cleansed_email_df.drop(dropping, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8517 entries, 0 to 9349\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   index                  8517 non-null   object \n",
      " 1   target                 8517 non-null   object \n",
      " 2   from                   8517 non-null   object \n",
      " 3   to                     8517 non-null   object \n",
      " 4   subject                8517 non-null   object \n",
      " 5   body                   8517 non-null   object \n",
      " 6   to_fullname            2403 non-null   object \n",
      " 7   to_email               8517 non-null   object \n",
      " 8   to_email_count         8517 non-null   float64\n",
      " 9   to_username            8517 non-null   object \n",
      " 10  to_domain              8517 non-null   object \n",
      " 11  from_fullname          7213 non-null   object \n",
      " 12  from_email             8517 non-null   object \n",
      " 13  from_email_count       8517 non-null   float64\n",
      " 14  from_username          8517 non-null   object \n",
      " 15  from_domain            8517 non-null   object \n",
      " 16  content-type-format    7646 non-null   object \n",
      " 17  content-type-type      7646 non-null   object \n",
      " 18  content-type-charset   5359 non-null   object \n",
      " 19  content-type-encoding  1262 non-null   object \n",
      "dtypes: float64(2), object(18)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cleansed_email_df.info())\n",
    "df = deepcopy(cleansed_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(df):\n",
    "    \"\"\"\n",
    "    Function to encapsulate the process of generating a list of unique words from the body column.\n",
    "    Input:\n",
    "        df: target dataframe\n",
    "    Output:\n",
    "        unique_words: a list of all unique words contained in the body column of the target dataframe df.\n",
    "    \"\"\"\n",
    "    punc_map = str.maketrans(dict.fromkeys(string.punctuation, ''))\n",
    "    df['words'] = df['body'].str.translate(punc_map).str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.split(\" \")\n",
    "    \n",
    "    unique_words = list(filter(None, (set([a.strip().lower() for b in df['words'].tolist() for a in b]))))\n",
    "    \n",
    "    unique_words_filtered = [word for word in unique_words if ('href' not in word and 'http' not in word)]\n",
    "    unique_words_filtered = [word for word in unique_words_filtered if not word.isnumeric()]\n",
    "    \n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    unique_words_us = [word for word in unique_words_filtered if d.check(word)]\n",
    "    \n",
    "    d = enchant.Dict(\"en_GB\")\n",
    "    unique_words_gb = [word for word in unique_words_filtered if d.check(word)]\n",
    "    \n",
    "    all_words = list(set(unique_words_gb + unique_words_us))\n",
    "    \n",
    "#     all_words = df[['body']].drop_duplicates()\n",
    "#     all_words['split'] = all_words['body'].astype(str).map(lambda x: x.split(\" \"))\n",
    "#     unique_words = (list(set([a.strip() for b in all_words['split'].tolist() for a in b])))\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_words_row(df_row):\n",
    "    return [a.strip() for a in re.split(r'[\\s,\\n\\.]+', df_row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_map(words, row):\n",
    "    \"\"\"\n",
    "    Passing in a list of unique cuisines that has been generated previously.\n",
    "    We compare the cuisines in a particular row and return a binary list of 0 for False, 1 for True in the cuisines is\n",
    "    in the row values.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [(a in row)*1 for a in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words_df(df):\n",
    "    df_full = pd.DataFrame(df)\n",
    "    df_full = df_full[df_full['body'].notna()]\n",
    "    #print(df_full.head())\n",
    "    try:\n",
    "        df_full.reset_index(inplace = True)\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        print(\"Index not reset\")\n",
    "    mapping_dict = {}\n",
    "    unique_words = get_unique_words(df_full)\n",
    "    for i in tqdm(range(df_full.shape[0])):\n",
    "        try:\n",
    "            df_row = prep_words_row(df_full['body'][i])\n",
    "        except:\n",
    "            pass\n",
    "#             print(df_full['body'][i])\n",
    "#             print(type(df_full['body'][i]))\n",
    "        word_map = get_word_map(unique_words, df_row)\n",
    "        mapping_dict[df_full['index'][i]] = word_map\n",
    "    word_mapping_df = pd.DataFrame.from_dict(mapping_dict, orient = 'index', columns = unique_words)\n",
    "    return word_mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8517/8517 [33:11<00:00,  4.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debit</th>\n",
       "      <th>blending</th>\n",
       "      <th>girlfriend</th>\n",
       "      <th>degradations</th>\n",
       "      <th>marks</th>\n",
       "      <th>pernicious</th>\n",
       "      <th>ritual</th>\n",
       "      <th>bumbling</th>\n",
       "      <th>limbo</th>\n",
       "      <th>monikers</th>\n",
       "      <th>...</th>\n",
       "      <th>perceptions</th>\n",
       "      <th>humanity</th>\n",
       "      <th>excruciating</th>\n",
       "      <th>carnivore</th>\n",
       "      <th>nights</th>\n",
       "      <th>aspen</th>\n",
       "      <th>tar</th>\n",
       "      <th>circumvents</th>\n",
       "      <th>forbidding</th>\n",
       "      <th>delinquent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001.ea7e79d3153e7469e7a9c3e0af6a357e</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003.acfc5ad94bbd27118a0d8685d18c89dd</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005.8c3b9e9c0f3f183ddaf7592a11b99957</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0006.ee8b0dba12856155222be180ba122058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0007.c75188382f64b090022fa3b095b020b0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01396.e80a10644810bc2ae3c1b58c5fd38dfa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01397.f75f0dd0dd923faefa3e9cc5ecb8c906</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01398.8ca7045aae4184d56e8509dc5ad6d979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01399.2319643317e2c5193d574e40a71809c2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01400.b444b69845db2fa0a4693ca04e6ac5c5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8517 rows × 24243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        debit  blending  girlfriend  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e       0         0           0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd       0         0           0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957       0         0           0   \n",
       "0006.ee8b0dba12856155222be180ba122058       0         0           0   \n",
       "0007.c75188382f64b090022fa3b095b020b0       0         0           0   \n",
       "...                                       ...       ...         ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa      0         0           0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906      0         0           0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979      0         0           0   \n",
       "01399.2319643317e2c5193d574e40a71809c2      0         0           0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5      0         0           0   \n",
       "\n",
       "                                        degradations  marks  pernicious  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e              0      0           0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd              0      0           0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957              0      0           0   \n",
       "0006.ee8b0dba12856155222be180ba122058              0      0           0   \n",
       "0007.c75188382f64b090022fa3b095b020b0              0      0           0   \n",
       "...                                              ...    ...         ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa             0      0           0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906             0      0           0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979             0      0           0   \n",
       "01399.2319643317e2c5193d574e40a71809c2             0      0           0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5             0      0           0   \n",
       "\n",
       "                                        ritual  bumbling  limbo  monikers  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e        0         0      0         0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd        0         0      0         0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957        0         0      0         0   \n",
       "0006.ee8b0dba12856155222be180ba122058        0         0      0         0   \n",
       "0007.c75188382f64b090022fa3b095b020b0        0         0      0         0   \n",
       "...                                        ...       ...    ...       ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa       0         0      0         0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906       0         0      0         0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979       0         0      0         0   \n",
       "01399.2319643317e2c5193d574e40a71809c2       0         0      0         0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5       0         0      0         0   \n",
       "\n",
       "                                        ...  perceptions  humanity  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e   ...            0         0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd   ...            0         0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957   ...            0         0   \n",
       "0006.ee8b0dba12856155222be180ba122058   ...            0         0   \n",
       "0007.c75188382f64b090022fa3b095b020b0   ...            0         0   \n",
       "...                                     ...          ...       ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa  ...            0         0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906  ...            0         0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979  ...            0         0   \n",
       "01399.2319643317e2c5193d574e40a71809c2  ...            0         0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5  ...            0         0   \n",
       "\n",
       "                                        excruciating  carnivore  nights  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e              0          0       0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd              0          0       0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957              0          0       0   \n",
       "0006.ee8b0dba12856155222be180ba122058              0          0       0   \n",
       "0007.c75188382f64b090022fa3b095b020b0              0          0       0   \n",
       "...                                              ...        ...     ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa             0          0       0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906             0          0       0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979             0          0       0   \n",
       "01399.2319643317e2c5193d574e40a71809c2             0          0       0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5             0          0       0   \n",
       "\n",
       "                                        aspen  tar  circumvents  forbidding  \\\n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e       0    0            0           0   \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd       0    0            0           0   \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957       0    0            0           0   \n",
       "0006.ee8b0dba12856155222be180ba122058       0    0            0           0   \n",
       "0007.c75188382f64b090022fa3b095b020b0       0    0            0           0   \n",
       "...                                       ...  ...          ...         ...   \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa      0    0            0           0   \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906      0    0            0           0   \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979      0    0            0           0   \n",
       "01399.2319643317e2c5193d574e40a71809c2      0    0            0           0   \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5      0    0            0           0   \n",
       "\n",
       "                                        delinquent  \n",
       "0001.ea7e79d3153e7469e7a9c3e0af6a357e            0  \n",
       "0003.acfc5ad94bbd27118a0d8685d18c89dd            0  \n",
       "0005.8c3b9e9c0f3f183ddaf7592a11b99957            0  \n",
       "0006.ee8b0dba12856155222be180ba122058            0  \n",
       "0007.c75188382f64b090022fa3b095b020b0            0  \n",
       "...                                            ...  \n",
       "01396.e80a10644810bc2ae3c1b58c5fd38dfa           0  \n",
       "01397.f75f0dd0dd923faefa3e9cc5ecb8c906           0  \n",
       "01398.8ca7045aae4184d56e8509dc5ad6d979           0  \n",
       "01399.2319643317e2c5193d574e40a71809c2           0  \n",
       "01400.b444b69845db2fa0a4693ca04e6ac5c5           0  \n",
       "\n",
       "[8517 rows x 24243 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_words_df(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
